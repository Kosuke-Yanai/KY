{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.1 過学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.12, test acc:0.112\n",
      "epoch:1, train acc:0.12333333333333334, test acc:0.1131\n",
      "epoch:2, train acc:0.13, test acc:0.1131\n",
      "epoch:3, train acc:0.13, test acc:0.1175\n",
      "epoch:4, train acc:0.14333333333333334, test acc:0.1228\n",
      "epoch:5, train acc:0.14333333333333334, test acc:0.1228\n",
      "epoch:6, train acc:0.16333333333333333, test acc:0.1281\n",
      "epoch:7, train acc:0.16333333333333333, test acc:0.1334\n",
      "epoch:8, train acc:0.17, test acc:0.1381\n",
      "epoch:9, train acc:0.17666666666666667, test acc:0.1372\n",
      "epoch:10, train acc:0.2, test acc:0.1436\n",
      "epoch:11, train acc:0.20333333333333334, test acc:0.1459\n",
      "epoch:12, train acc:0.23666666666666666, test acc:0.1588\n",
      "epoch:13, train acc:0.25, test acc:0.1649\n",
      "epoch:14, train acc:0.26, test acc:0.1721\n",
      "epoch:15, train acc:0.2633333333333333, test acc:0.1715\n",
      "epoch:16, train acc:0.26666666666666666, test acc:0.1725\n",
      "epoch:17, train acc:0.28, test acc:0.1768\n",
      "epoch:18, train acc:0.28, test acc:0.1774\n",
      "epoch:19, train acc:0.31333333333333335, test acc:0.1917\n",
      "epoch:20, train acc:0.32666666666666666, test acc:0.1978\n",
      "epoch:21, train acc:0.34, test acc:0.2113\n",
      "epoch:22, train acc:0.3566666666666667, test acc:0.2214\n",
      "epoch:23, train acc:0.41, test acc:0.2372\n",
      "epoch:24, train acc:0.43333333333333335, test acc:0.258\n",
      "epoch:25, train acc:0.43, test acc:0.264\n",
      "epoch:26, train acc:0.47333333333333333, test acc:0.2846\n",
      "epoch:27, train acc:0.5233333333333333, test acc:0.3287\n",
      "epoch:28, train acc:0.5633333333333334, test acc:0.358\n",
      "epoch:29, train acc:0.5866666666666667, test acc:0.3811\n",
      "epoch:30, train acc:0.6, test acc:0.4197\n",
      "epoch:31, train acc:0.63, test acc:0.4343\n",
      "epoch:32, train acc:0.6633333333333333, test acc:0.4545\n",
      "epoch:33, train acc:0.6866666666666666, test acc:0.4599\n",
      "epoch:34, train acc:0.6966666666666667, test acc:0.4932\n",
      "epoch:35, train acc:0.7133333333333334, test acc:0.512\n",
      "epoch:36, train acc:0.74, test acc:0.5336\n",
      "epoch:37, train acc:0.7433333333333333, test acc:0.5417\n",
      "epoch:38, train acc:0.77, test acc:0.5647\n",
      "epoch:39, train acc:0.8, test acc:0.5708\n",
      "epoch:40, train acc:0.8066666666666666, test acc:0.5891\n",
      "epoch:41, train acc:0.8, test acc:0.6017\n",
      "epoch:42, train acc:0.82, test acc:0.6027\n",
      "epoch:43, train acc:0.83, test acc:0.6056\n",
      "epoch:44, train acc:0.8466666666666667, test acc:0.6144\n",
      "epoch:45, train acc:0.84, test acc:0.6108\n",
      "epoch:46, train acc:0.8433333333333334, test acc:0.6187\n",
      "epoch:47, train acc:0.8366666666666667, test acc:0.6197\n",
      "epoch:48, train acc:0.84, test acc:0.6231\n",
      "epoch:49, train acc:0.86, test acc:0.6415\n",
      "epoch:50, train acc:0.8633333333333333, test acc:0.6447\n",
      "epoch:51, train acc:0.87, test acc:0.6403\n",
      "epoch:52, train acc:0.8833333333333333, test acc:0.6448\n",
      "epoch:53, train acc:0.87, test acc:0.6448\n",
      "epoch:54, train acc:0.8866666666666667, test acc:0.6572\n",
      "epoch:55, train acc:0.8966666666666666, test acc:0.6565\n",
      "epoch:56, train acc:0.8766666666666667, test acc:0.651\n",
      "epoch:57, train acc:0.8933333333333333, test acc:0.6599\n",
      "epoch:58, train acc:0.8866666666666667, test acc:0.6603\n",
      "epoch:59, train acc:0.9033333333333333, test acc:0.6644\n",
      "epoch:60, train acc:0.8933333333333333, test acc:0.6698\n",
      "epoch:61, train acc:0.9, test acc:0.6661\n",
      "epoch:62, train acc:0.9033333333333333, test acc:0.6674\n",
      "epoch:63, train acc:0.9066666666666666, test acc:0.6714\n",
      "epoch:64, train acc:0.9133333333333333, test acc:0.6764\n",
      "epoch:65, train acc:0.9166666666666666, test acc:0.676\n",
      "epoch:66, train acc:0.9233333333333333, test acc:0.6694\n",
      "epoch:67, train acc:0.9133333333333333, test acc:0.6796\n",
      "epoch:68, train acc:0.9166666666666666, test acc:0.6794\n",
      "epoch:69, train acc:0.93, test acc:0.6831\n",
      "epoch:70, train acc:0.93, test acc:0.6799\n",
      "epoch:71, train acc:0.93, test acc:0.6843\n",
      "epoch:72, train acc:0.9366666666666666, test acc:0.6893\n",
      "epoch:73, train acc:0.93, test acc:0.6797\n",
      "epoch:74, train acc:0.9333333333333333, test acc:0.6914\n",
      "epoch:75, train acc:0.94, test acc:0.6935\n",
      "epoch:76, train acc:0.9366666666666666, test acc:0.6906\n",
      "epoch:77, train acc:0.9466666666666667, test acc:0.6973\n",
      "epoch:78, train acc:0.9533333333333334, test acc:0.6991\n",
      "epoch:79, train acc:0.9566666666666667, test acc:0.7014\n",
      "epoch:80, train acc:0.9566666666666667, test acc:0.7014\n",
      "epoch:81, train acc:0.9666666666666667, test acc:0.7056\n",
      "epoch:82, train acc:0.9666666666666667, test acc:0.7071\n",
      "epoch:83, train acc:0.9666666666666667, test acc:0.6995\n",
      "epoch:84, train acc:0.9633333333333334, test acc:0.6972\n",
      "epoch:85, train acc:0.9733333333333334, test acc:0.7054\n",
      "epoch:86, train acc:0.97, test acc:0.7099\n",
      "epoch:87, train acc:0.97, test acc:0.7048\n",
      "epoch:88, train acc:0.9666666666666667, test acc:0.7098\n",
      "epoch:89, train acc:0.9733333333333334, test acc:0.709\n",
      "epoch:90, train acc:0.9833333333333333, test acc:0.7147\n",
      "epoch:91, train acc:0.9766666666666667, test acc:0.7075\n",
      "epoch:92, train acc:0.9733333333333334, test acc:0.7128\n",
      "epoch:93, train acc:0.9766666666666667, test acc:0.7149\n",
      "epoch:94, train acc:0.9766666666666667, test acc:0.7111\n",
      "epoch:95, train acc:0.9766666666666667, test acc:0.717\n",
      "epoch:96, train acc:0.98, test acc:0.7192\n",
      "epoch:97, train acc:0.9866666666666667, test acc:0.7226\n",
      "epoch:98, train acc:0.9766666666666667, test acc:0.7142\n",
      "epoch:99, train acc:0.9833333333333333, test acc:0.7171\n",
      "epoch:100, train acc:0.9833333333333333, test acc:0.7205\n",
      "epoch:101, train acc:0.99, test acc:0.7213\n",
      "epoch:102, train acc:0.9866666666666667, test acc:0.7194\n",
      "epoch:103, train acc:0.99, test acc:0.7209\n",
      "epoch:104, train acc:0.99, test acc:0.7247\n",
      "epoch:105, train acc:0.99, test acc:0.7209\n",
      "epoch:106, train acc:0.9966666666666667, test acc:0.7233\n",
      "epoch:107, train acc:0.9966666666666667, test acc:0.7262\n",
      "epoch:108, train acc:0.9966666666666667, test acc:0.728\n",
      "epoch:109, train acc:0.9966666666666667, test acc:0.7283\n",
      "epoch:110, train acc:0.9966666666666667, test acc:0.7296\n",
      "epoch:111, train acc:0.9966666666666667, test acc:0.7271\n",
      "epoch:112, train acc:0.9966666666666667, test acc:0.7274\n",
      "epoch:113, train acc:0.9966666666666667, test acc:0.7279\n",
      "epoch:114, train acc:0.9933333333333333, test acc:0.7284\n",
      "epoch:115, train acc:0.9933333333333333, test acc:0.727\n",
      "epoch:116, train acc:0.9933333333333333, test acc:0.7275\n",
      "epoch:117, train acc:0.9966666666666667, test acc:0.7296\n",
      "epoch:118, train acc:0.9966666666666667, test acc:0.7328\n",
      "epoch:119, train acc:0.9966666666666667, test acc:0.7288\n",
      "epoch:120, train acc:0.9966666666666667, test acc:0.7279\n",
      "epoch:121, train acc:0.9966666666666667, test acc:0.7279\n",
      "epoch:122, train acc:1.0, test acc:0.7287\n",
      "epoch:123, train acc:1.0, test acc:0.7325\n",
      "epoch:124, train acc:0.9966666666666667, test acc:0.7331\n",
      "epoch:125, train acc:0.9966666666666667, test acc:0.7336\n",
      "epoch:126, train acc:0.9966666666666667, test acc:0.7331\n",
      "epoch:127, train acc:0.9966666666666667, test acc:0.7337\n",
      "epoch:128, train acc:1.0, test acc:0.7376\n",
      "epoch:129, train acc:1.0, test acc:0.7376\n",
      "epoch:130, train acc:1.0, test acc:0.7352\n",
      "epoch:131, train acc:1.0, test acc:0.7341\n",
      "epoch:132, train acc:1.0, test acc:0.7352\n",
      "epoch:133, train acc:0.9966666666666667, test acc:0.7367\n",
      "epoch:134, train acc:0.9966666666666667, test acc:0.738\n",
      "epoch:135, train acc:0.9966666666666667, test acc:0.7381\n",
      "epoch:136, train acc:1.0, test acc:0.7361\n",
      "epoch:137, train acc:1.0, test acc:0.7378\n",
      "epoch:138, train acc:1.0, test acc:0.7373\n",
      "epoch:139, train acc:1.0, test acc:0.7374\n",
      "epoch:140, train acc:1.0, test acc:0.7323\n",
      "epoch:141, train acc:1.0, test acc:0.7355\n",
      "epoch:142, train acc:1.0, test acc:0.7345\n",
      "epoch:143, train acc:1.0, test acc:0.7364\n",
      "epoch:144, train acc:1.0, test acc:0.7341\n",
      "epoch:145, train acc:1.0, test acc:0.7355\n",
      "epoch:146, train acc:1.0, test acc:0.7375\n",
      "epoch:147, train acc:1.0, test acc:0.7392\n",
      "epoch:148, train acc:1.0, test acc:0.7376\n",
      "epoch:149, train acc:1.0, test acc:0.7385\n",
      "epoch:150, train acc:1.0, test acc:0.7386\n",
      "epoch:151, train acc:1.0, test acc:0.7368\n",
      "epoch:152, train acc:1.0, test acc:0.7374\n",
      "epoch:153, train acc:1.0, test acc:0.7366\n",
      "epoch:154, train acc:1.0, test acc:0.7402\n",
      "epoch:155, train acc:1.0, test acc:0.7401\n",
      "epoch:156, train acc:1.0, test acc:0.7413\n",
      "epoch:157, train acc:1.0, test acc:0.7391\n",
      "epoch:158, train acc:1.0, test acc:0.74\n",
      "epoch:159, train acc:1.0, test acc:0.7416\n",
      "epoch:160, train acc:1.0, test acc:0.7406\n",
      "epoch:161, train acc:1.0, test acc:0.7402\n",
      "epoch:162, train acc:1.0, test acc:0.7417\n",
      "epoch:163, train acc:1.0, test acc:0.7385\n",
      "epoch:164, train acc:1.0, test acc:0.7403\n",
      "epoch:165, train acc:1.0, test acc:0.7367\n",
      "epoch:166, train acc:1.0, test acc:0.7389\n",
      "epoch:167, train acc:1.0, test acc:0.7389\n",
      "epoch:168, train acc:1.0, test acc:0.7397\n",
      "epoch:169, train acc:1.0, test acc:0.7406\n",
      "epoch:170, train acc:1.0, test acc:0.7389\n",
      "epoch:171, train acc:1.0, test acc:0.74\n",
      "epoch:172, train acc:1.0, test acc:0.7417\n",
      "epoch:173, train acc:1.0, test acc:0.7402\n",
      "epoch:174, train acc:1.0, test acc:0.7412\n",
      "epoch:175, train acc:1.0, test acc:0.7432\n",
      "epoch:176, train acc:1.0, test acc:0.7414\n",
      "epoch:177, train acc:1.0, test acc:0.7437\n",
      "epoch:178, train acc:1.0, test acc:0.7431\n",
      "epoch:179, train acc:1.0, test acc:0.7419\n",
      "epoch:180, train acc:1.0, test acc:0.7426\n",
      "epoch:181, train acc:1.0, test acc:0.7421\n",
      "epoch:182, train acc:1.0, test acc:0.7428\n",
      "epoch:183, train acc:1.0, test acc:0.7427\n",
      "epoch:184, train acc:1.0, test acc:0.7434\n",
      "epoch:185, train acc:1.0, test acc:0.7419\n",
      "epoch:186, train acc:1.0, test acc:0.7412\n",
      "epoch:187, train acc:1.0, test acc:0.7408\n",
      "epoch:188, train acc:1.0, test acc:0.7407\n",
      "epoch:189, train acc:1.0, test acc:0.7408\n",
      "epoch:190, train acc:1.0, test acc:0.7403\n",
      "epoch:191, train acc:1.0, test acc:0.7416\n",
      "epoch:192, train acc:1.0, test acc:0.7426\n",
      "epoch:193, train acc:1.0, test acc:0.7425\n",
      "epoch:194, train acc:1.0, test acc:0.7432\n",
      "epoch:195, train acc:1.0, test acc:0.7435\n",
      "epoch:196, train acc:1.0, test acc:0.7436\n",
      "epoch:197, train acc:1.0, test acc:0.7437\n",
      "epoch:198, train acc:1.0, test acc:0.7436\n",
      "epoch:199, train acc:1.0, test acc:0.7441\n",
      "epoch:200, train acc:1.0, test acc:0.7432\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "#weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.3 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __int__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def  forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - sself.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3122227697986077\n",
      "=== epoch:1, train acc:0.11, test acc:0.108 ===\n",
      "train loss:2.3157425052894776\n",
      "train loss:2.299307102837142\n",
      "train loss:2.292060116707774\n",
      "=== epoch:2, train acc:0.11666666666666667, test acc:0.1078 ===\n",
      "train loss:2.311044553669637\n",
      "train loss:2.307682671162792\n",
      "train loss:2.306248114112768\n",
      "=== epoch:3, train acc:0.11666666666666667, test acc:0.1089 ===\n",
      "train loss:2.3117608056607906\n",
      "train loss:2.3117224824346976\n",
      "train loss:2.3058737530533295\n",
      "=== epoch:4, train acc:0.11666666666666667, test acc:0.1097 ===\n",
      "train loss:2.3146319919895593\n",
      "train loss:2.293696467540228\n",
      "train loss:2.3079238587339423\n",
      "=== epoch:5, train acc:0.11666666666666667, test acc:0.1105 ===\n",
      "train loss:2.299995744123357\n",
      "train loss:2.298746061939697\n",
      "train loss:2.3060660375667026\n",
      "=== epoch:6, train acc:0.12333333333333334, test acc:0.1104 ===\n",
      "train loss:2.308332665695044\n",
      "train loss:2.313741263347175\n",
      "train loss:2.309617141299629\n",
      "=== epoch:7, train acc:0.13, test acc:0.1097 ===\n",
      "train loss:2.298212716638554\n",
      "train loss:2.306768961409667\n",
      "train loss:2.2970346419386662\n",
      "=== epoch:8, train acc:0.13, test acc:0.1098 ===\n",
      "train loss:2.303461179303856\n",
      "train loss:2.3076453269307624\n",
      "train loss:2.308093877821176\n",
      "=== epoch:9, train acc:0.12333333333333334, test acc:0.1093 ===\n",
      "train loss:2.302194471568546\n",
      "train loss:2.3117957253829866\n",
      "train loss:2.309351367811222\n",
      "=== epoch:10, train acc:0.12333333333333334, test acc:0.1091 ===\n",
      "train loss:2.3045769578080604\n",
      "train loss:2.3074754799183985\n",
      "train loss:2.302793883660617\n",
      "=== epoch:11, train acc:0.12, test acc:0.109 ===\n",
      "train loss:2.307921395854726\n",
      "train loss:2.2981839437309284\n",
      "train loss:2.304511378242016\n",
      "=== epoch:12, train acc:0.12, test acc:0.1079 ===\n",
      "train loss:2.2883662922961987\n",
      "train loss:2.2856074412015444\n",
      "train loss:2.296652563019668\n",
      "=== epoch:13, train acc:0.12333333333333334, test acc:0.1084 ===\n",
      "train loss:2.2807293068856405\n",
      "train loss:2.305909261361413\n",
      "train loss:2.28927629400946\n",
      "=== epoch:14, train acc:0.12333333333333334, test acc:0.1097 ===\n",
      "train loss:2.2946964420484837\n",
      "train loss:2.2951772658797185\n",
      "train loss:2.295067354435013\n",
      "=== epoch:15, train acc:0.12333333333333334, test acc:0.1096 ===\n",
      "train loss:2.290537546277708\n",
      "train loss:2.286700974819476\n",
      "train loss:2.2976197007229446\n",
      "=== epoch:16, train acc:0.12333333333333334, test acc:0.1102 ===\n",
      "train loss:2.2997155161169305\n",
      "train loss:2.2984511168986717\n",
      "train loss:2.2995565699126423\n",
      "=== epoch:17, train acc:0.12333333333333334, test acc:0.1094 ===\n",
      "train loss:2.294992529899742\n",
      "train loss:2.2850955745947625\n",
      "train loss:2.2967784333181127\n",
      "=== epoch:18, train acc:0.12666666666666668, test acc:0.11 ===\n",
      "train loss:2.2911730352120454\n",
      "train loss:2.2917266713288598\n",
      "train loss:2.291557294010808\n",
      "=== epoch:19, train acc:0.12333333333333334, test acc:0.1095 ===\n",
      "train loss:2.2914750118842364\n",
      "train loss:2.298416844348334\n",
      "train loss:2.2904662110804703\n",
      "=== epoch:20, train acc:0.12333333333333334, test acc:0.1098 ===\n",
      "train loss:2.29186104007286\n",
      "train loss:2.2959487072446922\n",
      "train loss:2.2943618107934483\n",
      "=== epoch:21, train acc:0.12333333333333334, test acc:0.1107 ===\n",
      "train loss:2.292441479094641\n",
      "train loss:2.293868544554597\n",
      "train loss:2.2972480197394414\n",
      "=== epoch:22, train acc:0.12333333333333334, test acc:0.1108 ===\n",
      "train loss:2.2927923258838376\n",
      "train loss:2.288575703692288\n",
      "train loss:2.2904022821014176\n",
      "=== epoch:23, train acc:0.12, test acc:0.1113 ===\n",
      "train loss:2.291041136777582\n",
      "train loss:2.2883124785746514\n",
      "train loss:2.2861756041285717\n",
      "=== epoch:24, train acc:0.12333333333333334, test acc:0.1138 ===\n",
      "train loss:2.290579248156787\n",
      "train loss:2.290931954535771\n",
      "train loss:2.2882114863689846\n",
      "=== epoch:25, train acc:0.12333333333333334, test acc:0.1157 ===\n",
      "train loss:2.291919589087893\n",
      "train loss:2.291724733394545\n",
      "train loss:2.2772887693191364\n",
      "=== epoch:26, train acc:0.13, test acc:0.1171 ===\n",
      "train loss:2.289339304401839\n",
      "train loss:2.2885651610465243\n",
      "train loss:2.286330450929075\n",
      "=== epoch:27, train acc:0.13666666666666666, test acc:0.1198 ===\n",
      "train loss:2.287520192203125\n",
      "train loss:2.287246226576604\n",
      "train loss:2.287335299668942\n",
      "=== epoch:28, train acc:0.13, test acc:0.1227 ===\n",
      "train loss:2.2865221632130077\n",
      "train loss:2.2796809414555925\n",
      "train loss:2.2906253464294797\n",
      "=== epoch:29, train acc:0.13, test acc:0.125 ===\n",
      "train loss:2.288887503993506\n",
      "train loss:2.2805967277715844\n",
      "train loss:2.286218178523484\n",
      "=== epoch:30, train acc:0.13333333333333333, test acc:0.1268 ===\n",
      "train loss:2.2797618990297055\n",
      "train loss:2.2974056365455398\n",
      "train loss:2.2795827403722284\n",
      "=== epoch:31, train acc:0.14, test acc:0.1304 ===\n",
      "train loss:2.2831318169515065\n",
      "train loss:2.285550244720222\n",
      "train loss:2.2811119517290255\n",
      "=== epoch:32, train acc:0.14333333333333334, test acc:0.1354 ===\n",
      "train loss:2.275306813166095\n",
      "train loss:2.282363695281626\n",
      "train loss:2.288486683814023\n",
      "=== epoch:33, train acc:0.14333333333333334, test acc:0.137 ===\n",
      "train loss:2.2797641048071604\n",
      "train loss:2.2836423965922648\n",
      "train loss:2.2716676937699303\n",
      "=== epoch:34, train acc:0.14333333333333334, test acc:0.1401 ===\n",
      "train loss:2.281766422189799\n",
      "train loss:2.2826607347048005\n",
      "train loss:2.2795864825113457\n",
      "=== epoch:35, train acc:0.15, test acc:0.1463 ===\n",
      "train loss:2.2895583426993786\n",
      "train loss:2.2766441720775066\n",
      "train loss:2.2853237682207963\n",
      "=== epoch:36, train acc:0.15333333333333332, test acc:0.151 ===\n",
      "train loss:2.2710491030299904\n",
      "train loss:2.28634642720356\n",
      "train loss:2.2766819246050543\n",
      "=== epoch:37, train acc:0.16333333333333333, test acc:0.1536 ===\n",
      "train loss:2.2749060734104822\n",
      "train loss:2.277527505154393\n",
      "train loss:2.272782429934079\n",
      "=== epoch:38, train acc:0.16666666666666666, test acc:0.1549 ===\n",
      "train loss:2.2738979587647057\n",
      "train loss:2.2738085130065646\n",
      "train loss:2.2722321652120976\n",
      "=== epoch:39, train acc:0.16333333333333333, test acc:0.1597 ===\n",
      "train loss:2.2852780032373192\n",
      "train loss:2.2641162403289505\n",
      "train loss:2.275475201282451\n",
      "=== epoch:40, train acc:0.15666666666666668, test acc:0.1596 ===\n",
      "train loss:2.274771244661576\n",
      "train loss:2.2756029607026638\n",
      "train loss:2.2770619418488156\n",
      "=== epoch:41, train acc:0.16333333333333333, test acc:0.1644 ===\n",
      "train loss:2.2734075965713583\n",
      "train loss:2.277495995206018\n",
      "train loss:2.278770886536934\n",
      "=== epoch:42, train acc:0.16333333333333333, test acc:0.1664 ===\n",
      "train loss:2.2753122414459277\n",
      "train loss:2.261021680549719\n",
      "train loss:2.2715025031656704\n",
      "=== epoch:43, train acc:0.16333333333333333, test acc:0.1648 ===\n",
      "train loss:2.2789220667009213\n",
      "train loss:2.27036410927774\n",
      "train loss:2.2697863818330553\n",
      "=== epoch:44, train acc:0.17, test acc:0.1696 ===\n",
      "train loss:2.270398058770032\n",
      "train loss:2.27126642968056\n",
      "train loss:2.277584376403156\n",
      "=== epoch:45, train acc:0.17666666666666667, test acc:0.1739 ===\n",
      "train loss:2.2677094850864177\n",
      "train loss:2.280820206192206\n",
      "train loss:2.275459829676731\n",
      "=== epoch:46, train acc:0.18333333333333332, test acc:0.1748 ===\n",
      "train loss:2.27555723296281\n",
      "train loss:2.2708698405623666\n",
      "train loss:2.2718900510651534\n",
      "=== epoch:47, train acc:0.18666666666666668, test acc:0.1762 ===\n",
      "train loss:2.2805460756988625\n",
      "train loss:2.2563770820047124\n",
      "train loss:2.275103825531412\n",
      "=== epoch:48, train acc:0.19, test acc:0.1808 ===\n",
      "train loss:2.271843340473075\n",
      "train loss:2.267004085870104\n",
      "train loss:2.2787588460630492\n",
      "=== epoch:49, train acc:0.19333333333333333, test acc:0.1813 ===\n",
      "train loss:2.272858167917376\n",
      "train loss:2.274968109456206\n",
      "train loss:2.2680717353322\n",
      "=== epoch:50, train acc:0.18333333333333332, test acc:0.1818 ===\n",
      "train loss:2.2833258565763637\n",
      "train loss:2.2683041394578156\n",
      "train loss:2.2732029031602474\n",
      "=== epoch:51, train acc:0.19666666666666666, test acc:0.1821 ===\n",
      "train loss:2.275258416970556\n",
      "train loss:2.259751431826457\n",
      "train loss:2.2771311133501877\n",
      "=== epoch:52, train acc:0.19666666666666666, test acc:0.1856 ===\n",
      "train loss:2.2609719599968248\n",
      "train loss:2.259812547058098\n",
      "train loss:2.2577068618120846\n",
      "=== epoch:53, train acc:0.2, test acc:0.1869 ===\n",
      "train loss:2.2716739868391596\n",
      "train loss:2.2625057036859095\n",
      "train loss:2.271576524819013\n",
      "=== epoch:54, train acc:0.19666666666666666, test acc:0.1898 ===\n",
      "train loss:2.2615426382596753\n",
      "train loss:2.255606573790763\n",
      "train loss:2.270991211874751\n",
      "=== epoch:55, train acc:0.2, test acc:0.1932 ===\n",
      "train loss:2.2698925788577493\n",
      "train loss:2.2706185934141545\n",
      "train loss:2.2590338864876474\n",
      "=== epoch:56, train acc:0.21, test acc:0.1986 ===\n",
      "train loss:2.271348393071969\n",
      "train loss:2.2637423012352347\n",
      "train loss:2.272913664084769\n",
      "=== epoch:57, train acc:0.22, test acc:0.2028 ===\n",
      "train loss:2.269002249586702\n",
      "train loss:2.2623791698524753\n",
      "train loss:2.258003278451824\n",
      "=== epoch:58, train acc:0.22666666666666666, test acc:0.2071 ===\n",
      "train loss:2.2703521843864762\n",
      "train loss:2.270097538492659\n",
      "train loss:2.2727235867824036\n",
      "=== epoch:59, train acc:0.24666666666666667, test acc:0.2134 ===\n",
      "train loss:2.2575152972801344\n",
      "train loss:2.2584577756028197\n",
      "train loss:2.247790796762642\n",
      "=== epoch:60, train acc:0.25666666666666665, test acc:0.2191 ===\n",
      "train loss:2.2767504362662687\n",
      "train loss:2.256613095145074\n",
      "train loss:2.2680428583910337\n",
      "=== epoch:61, train acc:0.26, test acc:0.2211 ===\n",
      "train loss:2.262842342650349\n",
      "train loss:2.253832116515478\n",
      "train loss:2.261086397927752\n",
      "=== epoch:62, train acc:0.26666666666666666, test acc:0.2234 ===\n",
      "train loss:2.2628116561505296\n",
      "train loss:2.260406056905773\n",
      "train loss:2.2552442063246874\n",
      "=== epoch:63, train acc:0.2633333333333333, test acc:0.2224 ===\n",
      "train loss:2.270581453575288\n",
      "train loss:2.2588707363644733\n",
      "train loss:2.250088248572508\n",
      "=== epoch:64, train acc:0.2733333333333333, test acc:0.2216 ===\n",
      "train loss:2.2610767037000046\n",
      "train loss:2.2689194835672293\n",
      "train loss:2.239609447149574\n",
      "=== epoch:65, train acc:0.27666666666666667, test acc:0.2249 ===\n",
      "train loss:2.2782933557746325\n",
      "train loss:2.2584128858855426\n",
      "train loss:2.266196371973925\n",
      "=== epoch:66, train acc:0.2966666666666667, test acc:0.2277 ===\n",
      "train loss:2.2665503674786334\n",
      "train loss:2.281131894620839\n",
      "train loss:2.246260176268721\n",
      "=== epoch:67, train acc:0.29333333333333333, test acc:0.2275 ===\n",
      "train loss:2.258936695662764\n",
      "train loss:2.240268390879224\n",
      "train loss:2.2467083479954617\n",
      "=== epoch:68, train acc:0.3, test acc:0.2326 ===\n",
      "train loss:2.2535635659702846\n",
      "train loss:2.2543661665550396\n",
      "train loss:2.2558088449854536\n",
      "=== epoch:69, train acc:0.30333333333333334, test acc:0.2336 ===\n",
      "train loss:2.2381031994926\n",
      "train loss:2.2541104153250466\n",
      "train loss:2.24518867563957\n",
      "=== epoch:70, train acc:0.31, test acc:0.2337 ===\n",
      "train loss:2.2548340436140784\n",
      "train loss:2.2436164806455956\n",
      "train loss:2.2493573638494597\n",
      "=== epoch:71, train acc:0.30666666666666664, test acc:0.2349 ===\n",
      "train loss:2.2532087647136727\n",
      "train loss:2.259487279583094\n",
      "train loss:2.2515636156889878\n",
      "=== epoch:72, train acc:0.30333333333333334, test acc:0.2373 ===\n",
      "train loss:2.2552181898737986\n",
      "train loss:2.274441672820481\n",
      "train loss:2.250326188017738\n",
      "=== epoch:73, train acc:0.3, test acc:0.2378 ===\n",
      "train loss:2.252637169206855\n",
      "train loss:2.2499291835132467\n",
      "train loss:2.2407003342239786\n",
      "=== epoch:74, train acc:0.30333333333333334, test acc:0.2398 ===\n",
      "train loss:2.2518292590374474\n",
      "train loss:2.2477339287723073\n",
      "train loss:2.232202560192782\n",
      "=== epoch:75, train acc:0.31333333333333335, test acc:0.2437 ===\n",
      "train loss:2.2514991850567263\n",
      "train loss:2.247958207508894\n",
      "train loss:2.2567024293137465\n",
      "=== epoch:76, train acc:0.31666666666666665, test acc:0.2454 ===\n",
      "train loss:2.2517603675207996\n",
      "train loss:2.2635764593928522\n",
      "train loss:2.240676213303114\n",
      "=== epoch:77, train acc:0.3233333333333333, test acc:0.2476 ===\n",
      "train loss:2.2411059790029633\n",
      "train loss:2.2558771807340485\n",
      "train loss:2.2599956352170505\n",
      "=== epoch:78, train acc:0.32, test acc:0.2518 ===\n",
      "train loss:2.2488618073133435\n",
      "train loss:2.2472051690153454\n",
      "train loss:2.239910800362903\n",
      "=== epoch:79, train acc:0.32, test acc:0.2513 ===\n",
      "train loss:2.262667046376578\n",
      "train loss:2.2602473596519674\n",
      "train loss:2.255546191754638\n",
      "=== epoch:80, train acc:0.32666666666666666, test acc:0.2544 ===\n",
      "train loss:2.2445294610465587\n",
      "train loss:2.2310200999482337\n",
      "train loss:2.2330139476123803\n",
      "=== epoch:81, train acc:0.34, test acc:0.2556 ===\n",
      "train loss:2.235986990427485\n",
      "train loss:2.243320791452279\n",
      "train loss:2.271453200434012\n",
      "=== epoch:82, train acc:0.34, test acc:0.2608 ===\n",
      "train loss:2.254527685374911\n",
      "train loss:2.2620167208856485\n",
      "train loss:2.2374455851387696\n",
      "=== epoch:83, train acc:0.34, test acc:0.2601 ===\n",
      "train loss:2.2497792024891115\n",
      "train loss:2.2397564727997374\n",
      "train loss:2.2403355625539327\n",
      "=== epoch:84, train acc:0.3466666666666667, test acc:0.2645 ===\n",
      "train loss:2.2379322190793443\n",
      "train loss:2.2408895612342503\n",
      "train loss:2.2460881457240536\n",
      "=== epoch:85, train acc:0.3433333333333333, test acc:0.2727 ===\n",
      "train loss:2.254196033352216\n",
      "train loss:2.234844515038171\n",
      "train loss:2.236317281673644\n",
      "=== epoch:86, train acc:0.3433333333333333, test acc:0.2728 ===\n",
      "train loss:2.2338053025450106\n",
      "train loss:2.2431034856874934\n",
      "train loss:2.2605545356814085\n",
      "=== epoch:87, train acc:0.34, test acc:0.2719 ===\n",
      "train loss:2.2415129860714056\n",
      "train loss:2.236524513568848\n",
      "train loss:2.232469475404825\n",
      "=== epoch:88, train acc:0.3433333333333333, test acc:0.2716 ===\n",
      "train loss:2.24906927205202\n",
      "train loss:2.2277835249072386\n",
      "train loss:2.2414288046111306\n",
      "=== epoch:89, train acc:0.35333333333333333, test acc:0.2724 ===\n",
      "train loss:2.2416385557648546\n",
      "train loss:2.2350972094432153\n",
      "train loss:2.2378139990615944\n",
      "=== epoch:90, train acc:0.35333333333333333, test acc:0.2747 ===\n",
      "train loss:2.259195538519989\n",
      "train loss:2.2389709817841648\n",
      "train loss:2.243636475266237\n",
      "=== epoch:91, train acc:0.3566666666666667, test acc:0.2793 ===\n",
      "train loss:2.2235015326714334\n",
      "train loss:2.239083173257253\n",
      "train loss:2.2331159567436085\n",
      "=== epoch:92, train acc:0.36666666666666664, test acc:0.2829 ===\n",
      "train loss:2.247445948665836\n",
      "train loss:2.239288522252042\n",
      "train loss:2.229052308380795\n",
      "=== epoch:93, train acc:0.36333333333333334, test acc:0.2834 ===\n",
      "train loss:2.244753802787443\n",
      "train loss:2.233801154435751\n",
      "train loss:2.2411559517591546\n",
      "=== epoch:94, train acc:0.36666666666666664, test acc:0.2867 ===\n",
      "train loss:2.211378365961411\n",
      "train loss:2.230852121268578\n",
      "train loss:2.2558942148356333\n",
      "=== epoch:95, train acc:0.36666666666666664, test acc:0.2889 ===\n",
      "train loss:2.2307003418404268\n",
      "train loss:2.2460773937543137\n",
      "train loss:2.2289463243407632\n",
      "=== epoch:96, train acc:0.36666666666666664, test acc:0.2899 ===\n",
      "train loss:2.241222646892652\n",
      "train loss:2.2279411274963237\n",
      "train loss:2.2468851470243445\n",
      "=== epoch:97, train acc:0.37333333333333335, test acc:0.2928 ===\n",
      "train loss:2.231346772105619\n",
      "train loss:2.223512273048707\n",
      "train loss:2.228217295147695\n",
      "=== epoch:98, train acc:0.37333333333333335, test acc:0.294 ===\n",
      "train loss:2.236831443224333\n",
      "train loss:2.2321906074575835\n",
      "train loss:2.2357962128637787\n",
      "=== epoch:99, train acc:0.36666666666666664, test acc:0.29 ===\n",
      "train loss:2.226509427567203\n",
      "train loss:2.249394521519581\n",
      "train loss:2.210995452858721\n",
      "=== epoch:100, train acc:0.37333333333333335, test acc:0.2929 ===\n",
      "train loss:2.2457269956850405\n",
      "train loss:2.218855052229238\n",
      "train loss:2.2472019119024105\n",
      "=== epoch:101, train acc:0.37666666666666665, test acc:0.2945 ===\n",
      "train loss:2.2340637168836426\n",
      "train loss:2.2242649222775754\n",
      "train loss:2.2244892899136146\n",
      "=== epoch:102, train acc:0.37333333333333335, test acc:0.2916 ===\n",
      "train loss:2.2188159066666233\n",
      "train loss:2.2443392620910574\n",
      "train loss:2.235368680488778\n",
      "=== epoch:103, train acc:0.38333333333333336, test acc:0.2953 ===\n",
      "train loss:2.220819545347808\n",
      "train loss:2.2206849843261063\n",
      "train loss:2.219167059370251\n",
      "=== epoch:104, train acc:0.38, test acc:0.2975 ===\n",
      "train loss:2.2239137906162494\n",
      "train loss:2.2280851508644632\n",
      "train loss:2.2233794246896696\n",
      "=== epoch:105, train acc:0.4, test acc:0.3025 ===\n",
      "train loss:2.225831602093027\n",
      "train loss:2.206933208730327\n",
      "train loss:2.2075647208582736\n",
      "=== epoch:106, train acc:0.4, test acc:0.3042 ===\n",
      "train loss:2.232918564367162\n",
      "train loss:2.221262515163728\n",
      "train loss:2.220508529471067\n",
      "=== epoch:107, train acc:0.39, test acc:0.3001 ===\n",
      "train loss:2.226523728847123\n",
      "train loss:2.2254306381445086\n",
      "train loss:2.206894495204024\n",
      "=== epoch:108, train acc:0.39, test acc:0.301 ===\n",
      "train loss:2.204118871402834\n",
      "train loss:2.2380008791354746\n",
      "train loss:2.220389588659666\n",
      "=== epoch:109, train acc:0.39, test acc:0.3014 ===\n",
      "train loss:2.1938862245460755\n",
      "train loss:2.204439497931591\n",
      "train loss:2.2331391838862227\n",
      "=== epoch:110, train acc:0.4033333333333333, test acc:0.3077 ===\n",
      "train loss:2.2311475244966004\n",
      "train loss:2.225121140982594\n",
      "train loss:2.2119407742084336\n",
      "=== epoch:111, train acc:0.41, test acc:0.3086 ===\n",
      "train loss:2.2139798322689064\n",
      "train loss:2.2105286191677296\n",
      "train loss:2.210763165666515\n",
      "=== epoch:112, train acc:0.4033333333333333, test acc:0.3077 ===\n",
      "train loss:2.184746749328194\n",
      "train loss:2.2179610372601863\n",
      "train loss:2.224067955972499\n",
      "=== epoch:113, train acc:0.4066666666666667, test acc:0.3042 ===\n",
      "train loss:2.224671098602297\n",
      "train loss:2.2187411183455863\n",
      "train loss:2.2187924942749824\n",
      "=== epoch:114, train acc:0.4166666666666667, test acc:0.307 ===\n",
      "train loss:2.2118930632542924\n",
      "train loss:2.209054311722065\n",
      "train loss:2.211793652077386\n",
      "=== epoch:115, train acc:0.41, test acc:0.3084 ===\n",
      "train loss:2.218127873302555\n",
      "train loss:2.2117401251476427\n",
      "train loss:2.2143683465736017\n",
      "=== epoch:116, train acc:0.42333333333333334, test acc:0.312 ===\n",
      "train loss:2.211059322223189\n",
      "train loss:2.2298825031573357\n",
      "train loss:2.209802292865694\n",
      "=== epoch:117, train acc:0.42333333333333334, test acc:0.3128 ===\n",
      "train loss:2.2042595276131247\n",
      "train loss:2.1847699709735746\n",
      "train loss:2.208827232446626\n",
      "=== epoch:118, train acc:0.41333333333333333, test acc:0.3107 ===\n",
      "train loss:2.209615196383851\n",
      "train loss:2.2103007408048607\n",
      "train loss:2.2025893691947545\n",
      "=== epoch:119, train acc:0.41333333333333333, test acc:0.3091 ===\n",
      "train loss:2.226577560759104\n",
      "train loss:2.200337964936695\n",
      "train loss:2.223630209513748\n",
      "=== epoch:120, train acc:0.41, test acc:0.3074 ===\n",
      "train loss:2.202988572400155\n",
      "train loss:2.209221721971179\n",
      "train loss:2.200931035840481\n",
      "=== epoch:121, train acc:0.4066666666666667, test acc:0.3064 ===\n",
      "train loss:2.1989430616489454\n",
      "train loss:2.1888797415731815\n",
      "train loss:2.18429689878845\n",
      "=== epoch:122, train acc:0.41333333333333333, test acc:0.3095 ===\n",
      "train loss:2.21276459237495\n",
      "train loss:2.2120522800858438\n",
      "train loss:2.1819410177225635\n",
      "=== epoch:123, train acc:0.42, test acc:0.3137 ===\n",
      "train loss:2.214078417000479\n",
      "train loss:2.205858247971936\n",
      "train loss:2.2083404405120195\n",
      "=== epoch:124, train acc:0.42, test acc:0.3164 ===\n",
      "train loss:2.206939229924059\n",
      "train loss:2.188237673314547\n",
      "train loss:2.2033474707084193\n",
      "=== epoch:125, train acc:0.43, test acc:0.32 ===\n",
      "train loss:2.2006294194581804\n",
      "train loss:2.1840879681222347\n",
      "train loss:2.198685825479664\n",
      "=== epoch:126, train acc:0.4166666666666667, test acc:0.3146 ===\n",
      "train loss:2.1963935980469564\n",
      "train loss:2.170903337937355\n",
      "train loss:2.1773091117466943\n",
      "=== epoch:127, train acc:0.41333333333333333, test acc:0.3163 ===\n",
      "train loss:2.1928451576008965\n",
      "train loss:2.1755932099722237\n",
      "train loss:2.1763135794152295\n",
      "=== epoch:128, train acc:0.42, test acc:0.3125 ===\n",
      "train loss:2.1734152331031256\n",
      "train loss:2.197473526503304\n",
      "train loss:2.183406938948241\n",
      "=== epoch:129, train acc:0.42, test acc:0.3125 ===\n",
      "train loss:2.1745049552805096\n",
      "train loss:2.1938689476502415\n",
      "train loss:2.2031525119841295\n",
      "=== epoch:130, train acc:0.4266666666666667, test acc:0.3176 ===\n",
      "train loss:2.1648960659368033\n",
      "train loss:2.170861060599723\n",
      "train loss:2.188524602433433\n",
      "=== epoch:131, train acc:0.4266666666666667, test acc:0.3161 ===\n",
      "train loss:2.208498943386662\n",
      "train loss:2.1963294294095883\n",
      "train loss:2.1920946172327773\n",
      "=== epoch:132, train acc:0.42333333333333334, test acc:0.3169 ===\n",
      "train loss:2.184611313171835\n",
      "train loss:2.1699595800802376\n",
      "train loss:2.1589045920222354\n",
      "=== epoch:133, train acc:0.4166666666666667, test acc:0.3145 ===\n",
      "train loss:2.178216399942587\n",
      "train loss:2.1845087418745903\n",
      "train loss:2.195001049667129\n",
      "=== epoch:134, train acc:0.42333333333333334, test acc:0.3216 ===\n",
      "train loss:2.161715482774802\n",
      "train loss:2.1763104599792977\n",
      "train loss:2.1867970678651916\n",
      "=== epoch:135, train acc:0.42333333333333334, test acc:0.324 ===\n",
      "train loss:2.180654277654419\n",
      "train loss:2.1664093996157474\n",
      "train loss:2.166349525267648\n",
      "=== epoch:136, train acc:0.4533333333333333, test acc:0.333 ===\n",
      "train loss:2.1641483531114187\n",
      "train loss:2.1883062207189847\n",
      "train loss:2.1556433485974167\n",
      "=== epoch:137, train acc:0.4633333333333333, test acc:0.3346 ===\n",
      "train loss:2.1725299468819004\n",
      "train loss:2.1703059524998993\n",
      "train loss:2.172731159047158\n",
      "=== epoch:138, train acc:0.46, test acc:0.3394 ===\n",
      "train loss:2.1657734348356854\n",
      "train loss:2.163567341074636\n",
      "train loss:2.17425942510492\n",
      "=== epoch:139, train acc:0.4633333333333333, test acc:0.3394 ===\n",
      "train loss:2.1497322566013106\n",
      "train loss:2.166250611959223\n",
      "train loss:2.1254931735795255\n",
      "=== epoch:140, train acc:0.45, test acc:0.3284 ===\n",
      "train loss:2.1822042436213347\n",
      "train loss:2.184204345292084\n",
      "train loss:2.139100702018885\n",
      "=== epoch:141, train acc:0.44, test acc:0.3287 ===\n",
      "train loss:2.1948626693481526\n",
      "train loss:2.162527695641643\n",
      "train loss:2.177733394679436\n",
      "=== epoch:142, train acc:0.45666666666666667, test acc:0.3356 ===\n",
      "train loss:2.163550733064573\n",
      "train loss:2.1855023031243617\n",
      "train loss:2.1420898238113906\n",
      "=== epoch:143, train acc:0.45666666666666667, test acc:0.3366 ===\n",
      "train loss:2.172479961466332\n",
      "train loss:2.1810305183292615\n",
      "train loss:2.140074680048947\n",
      "=== epoch:144, train acc:0.4533333333333333, test acc:0.3405 ===\n",
      "train loss:2.150345754010611\n",
      "train loss:2.1294847521851916\n",
      "train loss:2.126341669239685\n",
      "=== epoch:145, train acc:0.44666666666666666, test acc:0.3371 ===\n",
      "train loss:2.1694286410727277\n",
      "train loss:2.1705185031150545\n",
      "train loss:2.1687873826443806\n",
      "=== epoch:146, train acc:0.46, test acc:0.3449 ===\n",
      "train loss:2.1570883278879722\n",
      "train loss:2.143069989364646\n",
      "train loss:2.181576209397532\n",
      "=== epoch:147, train acc:0.46, test acc:0.3495 ===\n",
      "train loss:2.139548600712027\n",
      "train loss:2.155085776698821\n",
      "train loss:2.1318702467663857\n",
      "=== epoch:148, train acc:0.4633333333333333, test acc:0.3436 ===\n",
      "train loss:2.137436391322011\n",
      "train loss:2.160412766649766\n",
      "train loss:2.1471298120300157\n",
      "=== epoch:149, train acc:0.46, test acc:0.3433 ===\n",
      "train loss:2.140572338687025\n",
      "train loss:2.1630766839789977\n",
      "train loss:2.1268938988418804\n",
      "=== epoch:150, train acc:0.45666666666666667, test acc:0.3435 ===\n",
      "train loss:2.135269246508582\n",
      "train loss:2.13599187833665\n",
      "train loss:2.1664013923012386\n",
      "=== epoch:151, train acc:0.46, test acc:0.3432 ===\n",
      "train loss:2.1654910790784454\n",
      "train loss:2.125820834013954\n",
      "train loss:2.1468671276115145\n",
      "=== epoch:152, train acc:0.4533333333333333, test acc:0.3379 ===\n",
      "train loss:2.1490049365962682\n",
      "train loss:2.1021853127537193\n",
      "train loss:2.1300185566168675\n",
      "=== epoch:153, train acc:0.44666666666666666, test acc:0.3373 ===\n",
      "train loss:2.1561957564213587\n",
      "train loss:2.1369178715150166\n",
      "train loss:2.138283030395934\n",
      "=== epoch:154, train acc:0.4533333333333333, test acc:0.338 ===\n",
      "train loss:2.122279546624935\n",
      "train loss:2.1335849106355314\n",
      "train loss:2.111386067757843\n",
      "=== epoch:155, train acc:0.44666666666666666, test acc:0.336 ===\n",
      "train loss:2.1103339734643805\n",
      "train loss:2.126981368583396\n",
      "train loss:2.1341948630532124\n",
      "=== epoch:156, train acc:0.45, test acc:0.3395 ===\n",
      "train loss:2.1293800750270155\n",
      "train loss:2.155200635346978\n",
      "train loss:2.1173527764843896\n",
      "=== epoch:157, train acc:0.44, test acc:0.3354 ===\n",
      "train loss:2.1327791718114195\n",
      "train loss:2.132485237304932\n",
      "train loss:2.0889819307899717\n",
      "=== epoch:158, train acc:0.44666666666666666, test acc:0.3379 ===\n",
      "train loss:2.097467871285918\n",
      "train loss:2.0979482989950204\n",
      "train loss:2.1358332800604796\n",
      "=== epoch:159, train acc:0.43666666666666665, test acc:0.3366 ===\n",
      "train loss:2.096236074597271\n",
      "train loss:2.153196223000283\n",
      "train loss:2.137700751662146\n",
      "=== epoch:160, train acc:0.43666666666666665, test acc:0.3346 ===\n",
      "train loss:2.1782886417235945\n",
      "train loss:2.145413090812144\n",
      "train loss:2.138957768576015\n",
      "=== epoch:161, train acc:0.4633333333333333, test acc:0.3451 ===\n",
      "train loss:2.0954941203540374\n",
      "train loss:2.091469071410685\n",
      "train loss:2.1182943941265937\n",
      "=== epoch:162, train acc:0.45, test acc:0.3408 ===\n",
      "train loss:2.084809764579749\n",
      "train loss:2.1214445344614568\n",
      "train loss:2.090409819702172\n",
      "=== epoch:163, train acc:0.4533333333333333, test acc:0.3396 ===\n",
      "train loss:2.128224693392757\n",
      "train loss:2.1137190884047543\n",
      "train loss:2.0801356849552723\n",
      "=== epoch:164, train acc:0.45666666666666667, test acc:0.3422 ===\n",
      "train loss:2.09762557801822\n",
      "train loss:2.131645258446733\n",
      "train loss:2.0554685198191898\n",
      "=== epoch:165, train acc:0.4633333333333333, test acc:0.3418 ===\n",
      "train loss:2.102184750893015\n",
      "train loss:2.12634318043996\n",
      "train loss:2.0790188714084135\n",
      "=== epoch:166, train acc:0.4633333333333333, test acc:0.3402 ===\n",
      "train loss:2.114567710858125\n",
      "train loss:2.094869908521923\n",
      "train loss:2.1127338648088347\n",
      "=== epoch:167, train acc:0.4666666666666667, test acc:0.3424 ===\n",
      "train loss:2.0947519827418137\n",
      "train loss:2.1271524882162485\n",
      "train loss:2.078361539743787\n",
      "=== epoch:168, train acc:0.4766666666666667, test acc:0.3484 ===\n",
      "train loss:2.0801145776004013\n",
      "train loss:2.0786480525719564\n",
      "train loss:2.0965327343264324\n",
      "=== epoch:169, train acc:0.47, test acc:0.3481 ===\n",
      "train loss:2.033412163312476\n",
      "train loss:2.1011658579304564\n",
      "train loss:2.101146631812271\n",
      "=== epoch:170, train acc:0.4666666666666667, test acc:0.345 ===\n",
      "train loss:2.0139945566747914\n",
      "train loss:2.115349518772421\n",
      "train loss:2.0939115234505303\n",
      "=== epoch:171, train acc:0.4666666666666667, test acc:0.3464 ===\n",
      "train loss:2.0985228136129095\n",
      "train loss:2.0805388099394246\n",
      "train loss:2.0843565468579848\n",
      "=== epoch:172, train acc:0.47, test acc:0.3519 ===\n",
      "train loss:2.111263337684912\n",
      "train loss:2.0868663404187497\n",
      "train loss:2.10037371207943\n",
      "=== epoch:173, train acc:0.4666666666666667, test acc:0.3521 ===\n",
      "train loss:2.099043481559014\n",
      "train loss:2.077597603200917\n",
      "train loss:2.013947294788548\n",
      "=== epoch:174, train acc:0.4533333333333333, test acc:0.3481 ===\n",
      "train loss:2.123933609223243\n",
      "train loss:2.0698090187700924\n",
      "train loss:2.0225453247454377\n",
      "=== epoch:175, train acc:0.4633333333333333, test acc:0.3459 ===\n",
      "train loss:2.0481571528519154\n",
      "train loss:2.0918714809516334\n",
      "train loss:2.06640085764658\n",
      "=== epoch:176, train acc:0.4666666666666667, test acc:0.3541 ===\n",
      "train loss:2.081591087529937\n",
      "train loss:2.0729675163874877\n",
      "train loss:2.03427689693639\n",
      "=== epoch:177, train acc:0.47, test acc:0.3587 ===\n",
      "train loss:2.068772021400524\n",
      "train loss:2.030034681268029\n",
      "train loss:2.0772560246072254\n",
      "=== epoch:178, train acc:0.4666666666666667, test acc:0.3591 ===\n",
      "train loss:2.054778049651121\n",
      "train loss:2.0396022478499574\n",
      "train loss:2.0527420210431964\n",
      "=== epoch:179, train acc:0.48, test acc:0.3622 ===\n",
      "train loss:2.0781774433604356\n",
      "train loss:2.0455141872008977\n",
      "train loss:2.125828368637514\n",
      "=== epoch:180, train acc:0.4866666666666667, test acc:0.3665 ===\n",
      "train loss:2.056988274869126\n",
      "train loss:2.09603148412646\n",
      "train loss:2.038992866943146\n",
      "=== epoch:181, train acc:0.49, test acc:0.3703 ===\n",
      "train loss:2.0439969885485887\n",
      "train loss:2.032371466436703\n",
      "train loss:2.0516141359700177\n",
      "=== epoch:182, train acc:0.48333333333333334, test acc:0.366 ===\n",
      "train loss:2.06616034907859\n",
      "train loss:2.0075675909171036\n",
      "train loss:2.0777889683706716\n",
      "=== epoch:183, train acc:0.4766666666666667, test acc:0.3683 ===\n",
      "train loss:1.993786067404181\n",
      "train loss:2.089909662899766\n",
      "train loss:2.0682016959982312\n",
      "=== epoch:184, train acc:0.48333333333333334, test acc:0.3679 ===\n",
      "train loss:1.9876719671283127\n",
      "train loss:2.065188947660434\n",
      "train loss:2.0514845660010264\n",
      "=== epoch:185, train acc:0.48333333333333334, test acc:0.3664 ===\n",
      "train loss:2.069199273928586\n",
      "train loss:2.0696288430969996\n",
      "train loss:2.075778091767601\n",
      "=== epoch:186, train acc:0.49333333333333335, test acc:0.3766 ===\n",
      "train loss:2.0426507012878004\n",
      "train loss:1.9804959346563393\n",
      "train loss:2.0785013766165537\n",
      "=== epoch:187, train acc:0.49666666666666665, test acc:0.3816 ===\n",
      "train loss:2.071431420643014\n",
      "train loss:2.0190883959270995\n",
      "train loss:2.0264661290596093\n",
      "=== epoch:188, train acc:0.5, test acc:0.3814 ===\n",
      "train loss:2.01522461959215\n",
      "train loss:2.065252357199916\n",
      "train loss:1.9959620638406523\n",
      "=== epoch:189, train acc:0.5166666666666667, test acc:0.3833 ===\n",
      "train loss:2.0371519004883507\n",
      "train loss:1.9584600947773567\n",
      "train loss:2.0404757447525657\n",
      "=== epoch:190, train acc:0.5066666666666667, test acc:0.3857 ===\n",
      "train loss:1.9632305245769428\n",
      "train loss:2.0799707207454174\n",
      "train loss:2.0277148631588138\n",
      "=== epoch:191, train acc:0.51, test acc:0.3872 ===\n",
      "train loss:2.0223689190521257\n",
      "train loss:2.039905145737843\n",
      "train loss:2.00109576944862\n",
      "=== epoch:192, train acc:0.51, test acc:0.3927 ===\n",
      "train loss:2.072235201668466\n",
      "train loss:1.9905575751897726\n",
      "train loss:1.9962754424170757\n",
      "=== epoch:193, train acc:0.5066666666666667, test acc:0.3923 ===\n",
      "train loss:2.0720231352445384\n",
      "train loss:2.008755693983492\n",
      "train loss:2.0093415185005115\n",
      "=== epoch:194, train acc:0.5066666666666667, test acc:0.3919 ===\n",
      "train loss:2.0240862598975427\n",
      "train loss:2.0227416418590862\n",
      "train loss:1.95490346612796\n",
      "=== epoch:195, train acc:0.5066666666666667, test acc:0.3931 ===\n",
      "train loss:2.0187307344812058\n",
      "train loss:2.0243187334331947\n",
      "train loss:1.958176586485169\n",
      "=== epoch:196, train acc:0.51, test acc:0.3971 ===\n",
      "train loss:1.9846591137443519\n",
      "train loss:1.9791772664902083\n",
      "train loss:1.998403207770219\n",
      "=== epoch:197, train acc:0.51, test acc:0.3962 ===\n",
      "train loss:2.0118662970095698\n",
      "train loss:1.969808691882729\n",
      "train loss:1.938862372462214\n",
      "=== epoch:198, train acc:0.51, test acc:0.394 ===\n",
      "train loss:1.9821189398460988\n",
      "train loss:1.9976628409866526\n",
      "train loss:1.9546030643133065\n",
      "=== epoch:199, train acc:0.5133333333333333, test acc:0.3987 ===\n",
      "train loss:1.9753528465717605\n",
      "train loss:1.9326487518934903\n",
      "train loss:1.944226073835961\n",
      "=== epoch:200, train acc:0.5133333333333333, test acc:0.3974 ===\n",
      "train loss:1.9370413972906768\n",
      "train loss:1.873914123128493\n",
      "train loss:1.957955274085692\n",
      "=== epoch:201, train acc:0.5033333333333333, test acc:0.3921 ===\n",
      "train loss:1.9409389675215443\n",
      "train loss:1.979061043665992\n",
      "train loss:1.9001508822218767\n",
      "=== epoch:202, train acc:0.5066666666666667, test acc:0.3917 ===\n",
      "train loss:1.99674821283586\n",
      "train loss:1.895628846534043\n",
      "train loss:1.8565677532030929\n",
      "=== epoch:203, train acc:0.5, test acc:0.3892 ===\n",
      "train loss:1.8956308425135275\n",
      "train loss:1.9782917119075183\n",
      "train loss:1.906293771158103\n",
      "=== epoch:204, train acc:0.5166666666666667, test acc:0.3948 ===\n",
      "train loss:1.9296971161183132\n",
      "train loss:1.947729275686456\n",
      "train loss:1.903344371615267\n",
      "=== epoch:205, train acc:0.51, test acc:0.3927 ===\n",
      "train loss:1.9422687233203881\n",
      "train loss:1.9596981508971063\n",
      "train loss:1.8858974966438518\n",
      "=== epoch:206, train acc:0.5333333333333333, test acc:0.4036 ===\n",
      "train loss:2.0306882080407096\n",
      "train loss:1.9566244019331576\n",
      "train loss:1.9427604885634187\n",
      "=== epoch:207, train acc:0.5466666666666666, test acc:0.4142 ===\n",
      "train loss:1.9336337588067958\n",
      "train loss:1.9212942636380215\n",
      "train loss:1.9334841241077825\n",
      "=== epoch:208, train acc:0.5433333333333333, test acc:0.4163 ===\n",
      "train loss:1.936192893594471\n",
      "train loss:1.9470447753978548\n",
      "train loss:2.0286794311590617\n",
      "=== epoch:209, train acc:0.5533333333333333, test acc:0.423 ===\n",
      "train loss:1.8935727799573963\n",
      "train loss:1.9254247406056297\n",
      "train loss:1.8995571818373744\n",
      "=== epoch:210, train acc:0.5533333333333333, test acc:0.4221 ===\n",
      "train loss:1.9201368465662723\n",
      "train loss:1.9057455807679338\n",
      "train loss:2.014064328819815\n",
      "=== epoch:211, train acc:0.5766666666666667, test acc:0.4315 ===\n",
      "train loss:1.9349865531942063\n",
      "train loss:1.9266231435088579\n",
      "train loss:1.9004863685569022\n",
      "=== epoch:212, train acc:0.57, test acc:0.435 ===\n",
      "train loss:1.9683301519437464\n",
      "train loss:1.9475202114784012\n",
      "train loss:1.9089609097462514\n",
      "=== epoch:213, train acc:0.5766666666666667, test acc:0.4475 ===\n",
      "train loss:1.8932185903505365\n",
      "train loss:1.88427374411207\n",
      "train loss:1.959473423617229\n",
      "=== epoch:214, train acc:0.58, test acc:0.4517 ===\n",
      "train loss:1.893326356448989\n",
      "train loss:1.9342909306328524\n",
      "train loss:1.9294493467493152\n",
      "=== epoch:215, train acc:0.5766666666666667, test acc:0.4516 ===\n",
      "train loss:1.8927961958650354\n",
      "train loss:1.900098560353687\n",
      "train loss:1.8931940540802534\n",
      "=== epoch:216, train acc:0.58, test acc:0.4508 ===\n",
      "train loss:1.9058264669695362\n",
      "train loss:1.9284667889510532\n",
      "train loss:1.8937186471098055\n",
      "=== epoch:217, train acc:0.5766666666666667, test acc:0.4492 ===\n",
      "train loss:1.9185320592320985\n",
      "train loss:1.9094978419249664\n",
      "train loss:1.7975157582643526\n",
      "=== epoch:218, train acc:0.58, test acc:0.4508 ===\n",
      "train loss:1.8266735345649983\n",
      "train loss:1.9004721332655499\n",
      "train loss:1.7742923137114872\n",
      "=== epoch:219, train acc:0.5766666666666667, test acc:0.4495 ===\n",
      "train loss:1.8966059910019997\n",
      "train loss:1.8604897673649634\n",
      "train loss:1.7669910206031587\n",
      "=== epoch:220, train acc:0.58, test acc:0.4527 ===\n",
      "train loss:1.8464115170081468\n",
      "train loss:1.929358686927881\n",
      "train loss:1.8076380954756812\n",
      "=== epoch:221, train acc:0.5833333333333334, test acc:0.4547 ===\n",
      "train loss:1.739407014132371\n",
      "train loss:1.879182313682397\n",
      "train loss:1.893883580843344\n",
      "=== epoch:222, train acc:0.58, test acc:0.4532 ===\n",
      "train loss:1.9002054210073833\n",
      "train loss:1.8639247955716818\n",
      "train loss:1.8641266455770074\n",
      "=== epoch:223, train acc:0.58, test acc:0.4521 ===\n",
      "train loss:1.8088449992926845\n",
      "train loss:1.8459583400701383\n",
      "train loss:1.8711141259063242\n",
      "=== epoch:224, train acc:0.58, test acc:0.4541 ===\n",
      "train loss:1.8809551792834278\n",
      "train loss:1.8801359190208322\n",
      "train loss:1.822421368354132\n",
      "=== epoch:225, train acc:0.5833333333333334, test acc:0.4557 ===\n",
      "train loss:1.9053333526262293\n",
      "train loss:1.7427259792912004\n",
      "train loss:1.8880062460868774\n",
      "=== epoch:226, train acc:0.5833333333333334, test acc:0.4626 ===\n",
      "train loss:1.9326281139898955\n",
      "train loss:1.7981649813971634\n",
      "train loss:1.8934402283848788\n",
      "=== epoch:227, train acc:0.5933333333333334, test acc:0.4612 ===\n",
      "train loss:1.8407314879376762\n",
      "train loss:1.7901984007924308\n",
      "train loss:1.8598695774425353\n",
      "=== epoch:228, train acc:0.5933333333333334, test acc:0.4589 ===\n",
      "train loss:1.8057239867897146\n",
      "train loss:1.827818741178491\n",
      "train loss:1.898636879542786\n",
      "=== epoch:229, train acc:0.6033333333333334, test acc:0.4678 ===\n",
      "train loss:1.8352260985088755\n",
      "train loss:1.8307561788186952\n",
      "train loss:1.733433128191468\n",
      "=== epoch:230, train acc:0.6, test acc:0.4719 ===\n",
      "train loss:1.8032994392315762\n",
      "train loss:1.852167199491862\n",
      "train loss:1.8314249208855673\n",
      "=== epoch:231, train acc:0.6066666666666667, test acc:0.4729 ===\n",
      "train loss:1.7914824524221418\n",
      "train loss:1.8265527778401127\n",
      "train loss:1.735092706862485\n",
      "=== epoch:232, train acc:0.6033333333333334, test acc:0.4726 ===\n",
      "train loss:1.8455889542398507\n",
      "train loss:1.7759059000291055\n",
      "train loss:1.8076129244689323\n",
      "=== epoch:233, train acc:0.6066666666666667, test acc:0.4726 ===\n",
      "train loss:1.8188539946430506\n",
      "train loss:1.7344879056421\n",
      "train loss:1.7683587109623504\n",
      "=== epoch:234, train acc:0.6033333333333334, test acc:0.4686 ===\n",
      "train loss:1.8217629156836241\n",
      "train loss:1.6638932979345864\n",
      "train loss:1.782123808732294\n",
      "=== epoch:235, train acc:0.5966666666666667, test acc:0.4711 ===\n",
      "train loss:1.6898439748661465\n",
      "train loss:1.7749254220649382\n",
      "train loss:1.732288513902214\n",
      "=== epoch:236, train acc:0.59, test acc:0.4723 ===\n",
      "train loss:1.875157043234055\n",
      "train loss:1.791343955401976\n",
      "train loss:1.7281078670104666\n",
      "=== epoch:237, train acc:0.6, test acc:0.4765 ===\n",
      "train loss:1.8193609370103188\n",
      "train loss:1.7133440676478644\n",
      "train loss:1.8217868097440073\n",
      "=== epoch:238, train acc:0.6, test acc:0.4771 ===\n",
      "train loss:1.7750712767209844\n",
      "train loss:1.673477416435164\n",
      "train loss:1.7717720485889914\n",
      "=== epoch:239, train acc:0.61, test acc:0.4764 ===\n",
      "train loss:1.77434386696718\n",
      "train loss:1.6966262136512062\n",
      "train loss:1.7092715871395106\n",
      "=== epoch:240, train acc:0.6066666666666667, test acc:0.4737 ===\n",
      "train loss:1.7751140177822455\n",
      "train loss:1.6987900069887578\n",
      "train loss:1.7542587772025766\n",
      "=== epoch:241, train acc:0.6133333333333333, test acc:0.4737 ===\n",
      "train loss:1.7217470866056863\n",
      "train loss:1.7557921840769657\n",
      "train loss:1.7893566177435631\n",
      "=== epoch:242, train acc:0.6233333333333333, test acc:0.4772 ===\n",
      "train loss:1.7594382113640157\n",
      "train loss:1.710991303826885\n",
      "train loss:1.7479105260332353\n",
      "=== epoch:243, train acc:0.6233333333333333, test acc:0.4836 ===\n",
      "train loss:1.6665902953555158\n",
      "train loss:1.6769401401839223\n",
      "train loss:1.7086695410530042\n",
      "=== epoch:244, train acc:0.6233333333333333, test acc:0.4781 ===\n",
      "train loss:1.7999268789495348\n",
      "train loss:1.6767334089765216\n",
      "train loss:1.6884587823604524\n",
      "=== epoch:245, train acc:0.62, test acc:0.4826 ===\n",
      "train loss:1.7178680607780685\n",
      "train loss:1.831594945087585\n",
      "train loss:1.6077843841683914\n",
      "=== epoch:246, train acc:0.61, test acc:0.4775 ===\n",
      "train loss:1.7620003836258693\n",
      "train loss:1.6708368058331822\n",
      "train loss:1.7346305711217307\n",
      "=== epoch:247, train acc:0.6066666666666667, test acc:0.4793 ===\n",
      "train loss:1.7057890876792436\n",
      "train loss:1.5729541389181727\n",
      "train loss:1.5858610762963639\n",
      "=== epoch:248, train acc:0.61, test acc:0.4737 ===\n",
      "train loss:1.6391180703841386\n",
      "train loss:1.763039523714844\n",
      "train loss:1.6858512192902133\n",
      "=== epoch:249, train acc:0.62, test acc:0.4808 ===\n",
      "train loss:1.7106904912048875\n",
      "train loss:1.7281704471949215\n",
      "train loss:1.7172678081887744\n",
      "=== epoch:250, train acc:0.6166666666666667, test acc:0.4831 ===\n",
      "train loss:1.6581105901491466\n",
      "train loss:1.6543379895240522\n",
      "train loss:1.6631613165402672\n",
      "=== epoch:251, train acc:0.6366666666666667, test acc:0.4894 ===\n",
      "train loss:1.722497571505733\n",
      "train loss:1.675605997348558\n",
      "train loss:1.6158140835267978\n",
      "=== epoch:252, train acc:0.64, test acc:0.4875 ===\n",
      "train loss:1.6634124420493808\n",
      "train loss:1.6304653400660822\n",
      "train loss:1.706200935134376\n",
      "=== epoch:253, train acc:0.64, test acc:0.4954 ===\n",
      "train loss:1.6444160539443573\n",
      "train loss:1.6146842563605055\n",
      "train loss:1.7241572188144998\n",
      "=== epoch:254, train acc:0.6433333333333333, test acc:0.4983 ===\n",
      "train loss:1.7031754881848917\n",
      "train loss:1.568099279539233\n",
      "train loss:1.6599764678643114\n",
      "=== epoch:255, train acc:0.6333333333333333, test acc:0.4898 ===\n",
      "train loss:1.5176182694276954\n",
      "train loss:1.663806096574687\n",
      "train loss:1.5811364355065485\n",
      "=== epoch:256, train acc:0.6466666666666666, test acc:0.4903 ===\n",
      "train loss:1.6027657322866742\n",
      "train loss:1.738961073245629\n",
      "train loss:1.6737466578494837\n",
      "=== epoch:257, train acc:0.6466666666666666, test acc:0.4919 ===\n",
      "train loss:1.6898956961421856\n",
      "train loss:1.5744271200833606\n",
      "train loss:1.6190239902888592\n",
      "=== epoch:258, train acc:0.6566666666666666, test acc:0.497 ===\n",
      "train loss:1.6428169122209335\n",
      "train loss:1.548544706661833\n",
      "train loss:1.5660972280327852\n",
      "=== epoch:259, train acc:0.6566666666666666, test acc:0.4955 ===\n",
      "train loss:1.6505959766053033\n",
      "train loss:1.6905174827890108\n",
      "train loss:1.5422903160142938\n",
      "=== epoch:260, train acc:0.6666666666666666, test acc:0.503 ===\n",
      "train loss:1.679901953520104\n",
      "train loss:1.5102943359753396\n",
      "train loss:1.604230727832164\n",
      "=== epoch:261, train acc:0.6633333333333333, test acc:0.5004 ===\n",
      "train loss:1.5222709723362255\n",
      "train loss:1.6562765113632452\n",
      "train loss:1.652126369808553\n",
      "=== epoch:262, train acc:0.67, test acc:0.5035 ===\n",
      "train loss:1.5825794473451886\n",
      "train loss:1.610217871369334\n",
      "train loss:1.4801543344307035\n",
      "=== epoch:263, train acc:0.6666666666666666, test acc:0.5037 ===\n",
      "train loss:1.4929786086178836\n",
      "train loss:1.6062190026707632\n",
      "train loss:1.579161992355438\n",
      "=== epoch:264, train acc:0.6633333333333333, test acc:0.5071 ===\n",
      "train loss:1.5304024809059382\n",
      "train loss:1.555234394527992\n",
      "train loss:1.6392824495542369\n",
      "=== epoch:265, train acc:0.6633333333333333, test acc:0.5096 ===\n",
      "train loss:1.5937655251704639\n",
      "train loss:1.6496061301138045\n",
      "train loss:1.5161073259997753\n",
      "=== epoch:266, train acc:0.6666666666666666, test acc:0.5112 ===\n",
      "train loss:1.5447040356961088\n",
      "train loss:1.6128315469138628\n",
      "train loss:1.586927171857526\n",
      "=== epoch:267, train acc:0.67, test acc:0.5162 ===\n",
      "train loss:1.5909478129517354\n",
      "train loss:1.5041913150153932\n",
      "train loss:1.580636546984812\n",
      "=== epoch:268, train acc:0.67, test acc:0.5197 ===\n",
      "train loss:1.5439942213623457\n",
      "train loss:1.6763175908976766\n",
      "train loss:1.5260126784927337\n",
      "=== epoch:269, train acc:0.6733333333333333, test acc:0.5214 ===\n",
      "train loss:1.4647247837049349\n",
      "train loss:1.51013895664832\n",
      "train loss:1.3630078383262356\n",
      "=== epoch:270, train acc:0.67, test acc:0.519 ===\n",
      "train loss:1.4139722673957125\n",
      "train loss:1.5412753496544438\n",
      "train loss:1.4893617577495157\n",
      "=== epoch:271, train acc:0.69, test acc:0.5209 ===\n",
      "train loss:1.513688829492055\n",
      "train loss:1.453715252741438\n",
      "train loss:1.4084977385069428\n",
      "=== epoch:272, train acc:0.6866666666666666, test acc:0.5227 ===\n",
      "train loss:1.551896325111642\n",
      "train loss:1.5034726606382558\n",
      "train loss:1.541263351394631\n",
      "=== epoch:273, train acc:0.69, test acc:0.5233 ===\n",
      "train loss:1.5045706605103817\n",
      "train loss:1.5736393600940095\n",
      "train loss:1.4879741660511663\n",
      "=== epoch:274, train acc:0.6933333333333334, test acc:0.5265 ===\n",
      "train loss:1.4738920093492127\n",
      "train loss:1.5234014191999155\n",
      "train loss:1.4986426850930834\n",
      "=== epoch:275, train acc:0.7, test acc:0.5298 ===\n",
      "train loss:1.5708107497498418\n",
      "train loss:1.6454429326542845\n",
      "train loss:1.5062482082213071\n",
      "=== epoch:276, train acc:0.7, test acc:0.531 ===\n",
      "train loss:1.441472004149067\n",
      "train loss:1.549075009256223\n",
      "train loss:1.5319940336085514\n",
      "=== epoch:277, train acc:0.7, test acc:0.5325 ===\n",
      "train loss:1.6076423497390226\n",
      "train loss:1.5546104636447955\n",
      "train loss:1.5184882250990521\n",
      "=== epoch:278, train acc:0.7, test acc:0.5327 ===\n",
      "train loss:1.4658787038631673\n",
      "train loss:1.5776168946183602\n",
      "train loss:1.56067056479855\n",
      "=== epoch:279, train acc:0.7, test acc:0.5338 ===\n",
      "train loss:1.5457812813090717\n",
      "train loss:1.5019376673929818\n",
      "train loss:1.4266395996654035\n",
      "=== epoch:280, train acc:0.7033333333333334, test acc:0.5349 ===\n",
      "train loss:1.5263371586232355\n",
      "train loss:1.3827414011499461\n",
      "train loss:1.4865082281851882\n",
      "=== epoch:281, train acc:0.7033333333333334, test acc:0.5374 ===\n",
      "train loss:1.4560923547321858\n",
      "train loss:1.4034575766855846\n",
      "train loss:1.4363011348747614\n",
      "=== epoch:282, train acc:0.7033333333333334, test acc:0.5353 ===\n",
      "train loss:1.3535744075468374\n",
      "train loss:1.5949936552017892\n",
      "train loss:1.4928800288366264\n",
      "=== epoch:283, train acc:0.7066666666666667, test acc:0.5367 ===\n",
      "train loss:1.4441214100504252\n",
      "train loss:1.4475801407874678\n",
      "train loss:1.409203136613285\n",
      "=== epoch:284, train acc:0.7066666666666667, test acc:0.5337 ===\n",
      "train loss:1.4278471079206572\n",
      "train loss:1.5677987658951185\n",
      "train loss:1.495741613933086\n",
      "=== epoch:285, train acc:0.71, test acc:0.5413 ===\n",
      "train loss:1.4428533780644281\n",
      "train loss:1.4950302648286347\n",
      "train loss:1.3166796425417457\n",
      "=== epoch:286, train acc:0.71, test acc:0.5399 ===\n",
      "train loss:1.4851875890067925\n",
      "train loss:1.3542167615975353\n",
      "train loss:1.3742052443986512\n",
      "=== epoch:287, train acc:0.7133333333333334, test acc:0.5396 ===\n",
      "train loss:1.4912541148978782\n",
      "train loss:1.4657663396383709\n",
      "train loss:1.3669660334455533\n",
      "=== epoch:288, train acc:0.71, test acc:0.5432 ===\n",
      "train loss:1.464695329941064\n",
      "train loss:1.3151395435217503\n",
      "train loss:1.4245113715186017\n",
      "=== epoch:289, train acc:0.71, test acc:0.5376 ===\n",
      "train loss:1.4280405461149506\n",
      "train loss:1.460522559711926\n",
      "train loss:1.445335279693846\n",
      "=== epoch:290, train acc:0.7066666666666667, test acc:0.5311 ===\n",
      "train loss:1.2907214961308475\n",
      "train loss:1.408497006371935\n",
      "train loss:1.4913423124102971\n",
      "=== epoch:291, train acc:0.7166666666666667, test acc:0.5451 ===\n",
      "train loss:1.4114655808831897\n",
      "train loss:1.4146147834328695\n",
      "train loss:1.356728275529426\n",
      "=== epoch:292, train acc:0.7166666666666667, test acc:0.548 ===\n",
      "train loss:1.3538559194134718\n",
      "train loss:1.4428525435926867\n",
      "train loss:1.4100306288558349\n",
      "=== epoch:293, train acc:0.7166666666666667, test acc:0.5517 ===\n",
      "train loss:1.3987318205544381\n",
      "train loss:1.3156795330898199\n",
      "train loss:1.484971004859698\n",
      "=== epoch:294, train acc:0.7266666666666667, test acc:0.5499 ===\n",
      "train loss:1.360825472418365\n",
      "train loss:1.2767942183107568\n",
      "train loss:1.487113106578094\n",
      "=== epoch:295, train acc:0.7133333333333334, test acc:0.5491 ===\n",
      "train loss:1.3288872203873652\n",
      "train loss:1.389392974664763\n",
      "train loss:1.3761104755038729\n",
      "=== epoch:296, train acc:0.7266666666666667, test acc:0.5574 ===\n",
      "train loss:1.1747451896629513\n",
      "train loss:1.3489037149411396\n",
      "train loss:1.4031521775194744\n",
      "=== epoch:297, train acc:0.72, test acc:0.5584 ===\n",
      "train loss:1.4052294018974743\n",
      "train loss:1.3307367078497878\n",
      "train loss:1.2570108345939623\n",
      "=== epoch:298, train acc:0.7233333333333334, test acc:0.5551 ===\n",
      "train loss:1.3609666072143034\n",
      "train loss:1.3620737165582901\n",
      "train loss:1.4369303634809574\n",
      "=== epoch:299, train acc:0.72, test acc:0.5586 ===\n",
      "train loss:1.249467917839095\n",
      "train loss:1.395752430001706\n",
      "train loss:1.427581609947676\n",
      "=== epoch:300, train acc:0.7233333333333334, test acc:0.5666 ===\n",
      "train loss:1.4037774562545418\n",
      "train loss:1.2965059763614832\n",
      "train loss:1.2596077447325802\n",
      "=== epoch:301, train acc:0.73, test acc:0.5666 ===\n",
      "train loss:1.3834682410470511\n",
      "train loss:1.2016961199664613\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5666\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# Dropuoutの有無、割り合いの設定 ========================\n",
    "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.util import shuffle_dataset\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
